{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemGPT implementation  \n",
    "\n",
    "MemGPT implementation given in LangGraph tutorials, inspired by the paper cited in the [README](../README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4d1d95c-0c2e-493a-a067-0a4393475f3b\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "#Memories are namespaced by a tuple, which in this specific example will be (<user_id>, \"memories\")\n",
    "user_id = '1'\n",
    "namespace_for_memory = (user_id, 'memories')\n",
    "#We use the store.put method to save memories to our namespace in the store. When we do this, we specify \n",
    "#the namespace and a key-value pair for the memory. The key is a unique identifier, and the value\n",
    "#is the memory itself (a dictionary)\n",
    "memory_id = str(uuid.uuid4())\n",
    "print(memory_id)\n",
    "memory_1 = {\"food_preference: \" : \"I like pizza\"}\n",
    "#Wrong: the memory is associated to an id, by doing so, you are overwriting the previous memory!\n",
    "#Generate a new id (memory_id) each time\n",
    "memory_2 = {\"Best activity: \": \"I like to play basketball\"}\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory_1)\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['1', 'memories'], key='e4d1d95c-0c2e-493a-a067-0a4393475f3b', value={'Best activity: ': 'I like to play basketball'}, created_at='2025-03-21T10:35:44.279694+00:00', updated_at='2025-03-21T10:35:44.279694+00:00', score=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can read out memories in our namespace using store.search method\n",
    "#This method takes the namespace and returns a list of memories for that user\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "for mem in memories:\n",
    "    print(mem.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n",
    "        \"dims\": 1536,                              # Embedding dimensions\n",
    "        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find memories about food preferences\n",
    "# (This can be done after putting memories into the store)\n",
    "memories = store.search(\n",
    "    namespace_for_memory,\n",
    "    query=\"What does the user like to eat?\",\n",
    "    limit=3  # Return top 3 matches\n",
    ")\n",
    "memories[-1] #Obviously empty because we didn't put any memory in the store\n",
    "#We just defined the fields we want to embed, but we didn't put any memory in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store with specific fields to embed\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\n",
    "        \"food_preference\": \"I love Italian cuisine\",\n",
    "        \"context\": \"Discussing dinner plans\"\n",
    "    },\n",
    "    index=[\"food_preference\"]  # Only embed \"food_preferences\" field, context cannot be searched semantically\n",
    ")\n",
    "#You can save in another embedding also the context by doing: index=[\"food_preference\", \"context\"]\n",
    "#Is it useful? It depends, if statements are very generic, no.\n",
    "\n",
    "# Store without embedding (still retrievable, but not searchable)\n",
    "store.put(\n",
    "    namespace_for_memory,\n",
    "    str(uuid.uuid4()),\n",
    "    {\"system_info\": \"Last updated: 2024-01-01\"},\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Semantic search in a graph  \n",
    "\n",
    "Still working on a chatbot-style Graph, I didn't look yet at how to make it iterative. I am only trying to introduce the concept of memory and a context window that is token aware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Semantic search](https://langchain-ai.github.io/langgraph/cloud/deployment/semantic_search/)  \n",
    "\n",
    "[Deploy application](https://langchain-ai.github.io/langgraph/cloud/deployment/setup_pyproject/#specify-dependencies)  \n",
    "\n",
    "[Move to LangSmith](https://langchain-ai.github.io/langgraph/cloud/deployment/cloud/#create-new-deployment)  \n",
    "\n",
    "[Save memories](https://python.langchain.com/docs/versions/migrating_memory/long_term_memory_agent/)  \n",
    "\n",
    "[How to use tools](https://js.langchain.com/docs/how_to/tool_calling/#:~:text=Chat%20models%20that%20support%20tool%20calling%20features%20implement,tool%20schemas%20in%20its%20calls%20to%20the%20LLM.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating any LangGraph graph, you can set it up to persist its state by adding a checkpointer when compiling the graph. When you compile graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. In this case, **the entire history** is passed to the LLM, making it easy to go over the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need, on the other hand, is a prompt that is token aware and that is able to perform semantic search. Thus, in order to save memories, we can use InMemoryStore where data are stored in main memory (RAM) or on a database (not really necessary). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide a tool to the LLM to store relevant memories when it finds it is necessary. Then, when answering to a new prompt, it has the possibility to perform semantic search.   \n",
    "By following the approach of MemGPT, we divide the context window into three different sections:   \n",
    "1. System instructions;\n",
    "2. Working context;\n",
    "3. FIFO Queue;\n",
    "\n",
    "The system instructions are automatically managed by LangGraph when binding a tool to the agent, passing the LLM something like:   \n",
    "\n",
    "```\n",
    "You have access to the following tools:\n",
    "\n",
    "Tool name: search_memory\n",
    "Description: Look up relevant memories using semantic search.\n",
    "\n",
    "Input parameters:\n",
    "- query (string): \n",
    "- limit (integer): \n",
    "\n",
    "You can call a tool by responding with a JSON object like:\n",
    "{\n",
    "  \"tool\": \"search_memory\",\n",
    "  \"tool_input\": {\n",
    "    \"query\": \"your question\",\n",
    "    \"limit\": 3\n",
    "  }\n",
    "}\n",
    "```  \n",
    "We have to first understand what is the dimension of the context window and which model we want to use. Then, System informations can be measured precisely when we will have all the tools (Web search, memory and PCAP reader). Thus said, we have to decide which percentage of the remaining context window we want to assign to the FIFO queue containing the entire history of the conversation and which percentage of the context window. **I still have to understand how to limit the semantic search done by the LLM in memory.**  \n",
    "A possible idea could be:  \n",
    "1. Measure System information once you have all tools: it's fixed, and we call it total info;\n",
    "2. At the beginning, the FIFO queue has a dimension that is the same as (context window)-total info\n",
    "3. When we reach 85-90% of the FIFO capacity, we flush it back to 60%, save entirely what we removed from the FIFO into storage and assign a 20% of context window for data retrieved by the storage;\n",
    "4. Repeat the process when the FIFO reaches again 80% of capacity\n",
    "\n",
    "Problem: how do I manage this? I have to save info on my own, without requiring the LLM to do it, because it is based on the capacity of the FIFO. Then, I have to tell him to retrieve data from the db if necessary ONLY WHEN THERE IS SOMETHING STORED, but if I bind a tool for semantic search, then it will always be able to search in the db, even if empty.\n",
    "You can dynamically bind tools to the LLM:   \n",
    "```python  \n",
    "tools = []\n",
    "if memory_store.has_data():\n",
    "    tools.append(search_tool)\n",
    "\n",
    "llm = llm.bind_tools(tools)\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
